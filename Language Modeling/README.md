## Language Modeling

#### 2020

| Title | Summary | Paper Source |
| ----- | ------- | ----- |
| Reformer: The Efficient Transformer | [Pragmatic ML](https://www.pragmatic.ml/reformer-deep-dive/) | [Paper](https://arxiv.org/abs/2001.04451)
| ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators | [Google AI Blog](https://ai.googleblog.com/2020/03/more-efficient-nlp-model-pre-training.html) | [Paper](https://openreview.net/pdf?id=r1xMH1BtvB) |

#### 2019

| Title | Summary | Paper Source | TL;DR |
| ----- | ------- | ----- | ----- |
| Language Models are Unsupervised Multitask Learners | - | [Paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf) | [Richard Csaky](https://github.com/ricsinaruto/Seq2seqChatbots/wiki/Chatbot-and-Related-Research-Paper-Notes-with-Images#language-models-are-unsupervised-multitask-learners-s2s)
| Plug and Play Language Models: A Simple Approach to Controlled Text Generation | [Uber Engineering](https://eng.uber.com/pplm/) | [Paper](https://arxiv.org/abs/1912.02164)| - |
| ALBERT: A Lite BERT for Self-Supervised Learning Of Language Representations | [Amit Chaudhary](https://amitness.com/2020/02/albert-visual-summary/) | [Paper](https://arxiv.org/abs/1909.11942)| - |
| Fine-Tuning GPT-2 from Human Preferences | [Open AI](https://openai.com/blog/fine-tuning-gpt-2/) | [Paper](https://arxiv.org/abs/1909.08593)| - |
| XLNet: Generalized Autoregressive Pretraining for Language Understanding | [dair.ai](https://medium.com/dair-ai/xlnet-outperforms-bert-on-several-nlp-tasks-9ec867bb563b) | [Paper](https://arxiv.org/abs/1906.08237)| - |
| Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context | [dair.ai](https://medium.com/dair-ai/a-light-introduction-to-transformer-xl-be5737feb13) | [Paper](https://arxiv.org/abs/1901.02860) | - |
| Parameter-Efficient Transfer Learning for NLP | [dair.ai](https://medium.com/dair-ai/adapters-a-compact-and-extensible-transfer-learning-method-for-nlp-6d18c2399f62) | [Paper](https://arxiv.org/abs/1902.00751) | - |

#### 2018

| Title | Paper Source | TL;DR |
| ----- | ------- | ----- |
| BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding| [Paper](https://arxiv.org/pdf/1810.04805.pdf) | [Richard Csaky](https://github.com/ricsinaruto/Seq2seqChatbots/wiki/Chatbot-and-Related-Research-Paper-Notes-with-Images#bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding-s2s)

#### 2017

| Title | Paper Source | TL;DR |
| ----- | ------- | ----- |
| Adversarial Generation of Natural Language| [Paper](https://arxiv.org/pdf/1705.10929.pdf) | [Richard Csaky](https://github.com/ricsinaruto/Seq2seqChatbots/wiki/Chatbot-and-Related-Research-Paper-Notes-with-Images#adversarial-generation-of-natural-language-n-c)
| Training RNNs as Fast as CNNs| [Paper](https://arxiv.org/pdf/1709.02755.pdf) | [Richard Csaky](https://github.com/ricsinaruto/Seq2seqChatbots/wiki/Chatbot-and-Related-Research-Paper-Notes-with-Images#training-rnns-as-fast-as-cnns-n-c)
